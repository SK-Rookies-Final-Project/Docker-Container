===> User
uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
===> Configuring ...
Running in KRaft mode...
===> Running preflight checks ... 
===> Check if /var/lib/kafka/data is writable ...
===> Running in KRaft mode, skipping Zookeeper health check...
===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
Log directory /var/lib/kafka/data is already formatted. Use --ignore-formatted to ignore this directory and format the others.
===> Launching ... 
===> Launching kafka ... 
[2025-07-28 05:43:27,755] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-07-28 05:43:29,721] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2025-07-28 05:43:30,708] INFO KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 101
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = x63tRl4gS8-RYe9Jmf0NCw
	client.quota.callback.class = null
	client.quota.max.throttle.time.ms = 5000
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.api.visibility = DEFAULT
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
	confluent.balancer.capacity.threshold.upper.limit = 0.95
	confluent.balancer.cell.load.upper.bound = 0.7
	confluent.balancer.cell.overload.duration.ms = 86400000
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.cpu.balance.threshold = 1.1
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.enabled = false
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.enabled = false
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.min.valid.windows = 5
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.inter.cell.balancing.enabled = false
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.sbc.metrics.parser.enabled = false
	confluent.balancer.self.healing.maximum.rounds = 1
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.tenant.maximum.movements = 0
	confluent.balancer.tenant.suspension.ms = 86400000
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
	confluent.balancer.topic.partition.suspension.ms = 18000000
	confluent.balancer.topic.replication.factor = 3
	confluent.balancer.triggering.goals = []
	confluent.balancer.v2.addition.enabled = false
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.advertised.limit.load = 0.8
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.tenant.metric.enable = false
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.full.configs.enable = false
	confluent.catalog.collector.max.bytes.per.snapshot = 850000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.load.refresher.enable = true
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.admin.max.in.flight.requests = 1000
	confluent.cluster.link.admin.request.batch.size = 1
	confluent.cluster.link.allow.config.providers = true
	confluent.cluster.link.allow.legacy.message.format = false
	confluent.cluster.link.allow.truncation.below.hwm = true
	confluent.cluster.link.availability.check.mode = ALL
	confluent.cluster.link.background.thread.affinity = LINK
	confluent.cluster.link.clients.max.idle.ms = 3153600000000
	confluent.cluster.link.enable = true
	confluent.cluster.link.enable.local.admin = false
	confluent.cluster.link.enable.metrics.reduction = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.fetcher.auto.tune.enable = false
	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.local.reverse.connection.listener.map = null
	confluent.cluster.link.max.client.connections = 2147483647
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.mirror.transition.batch.size = 50
	confluent.cluster.link.num.background.threads = 1
	confluent.cluster.link.replica.fetch.connections.mode = combined
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.cluster.link.request.quota.capacity = 400
	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
	confluent.cluster.link.tenant.quotas.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.enable = false
	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
	confluent.cluster.metadata.snapshot.tier.upload.enable = false
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
	confluent.consumer.fetch.partition.pruning.enable = true
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.disk.io.manager.enable = false
	confluent.disk.throughput.headroom = 10485760
	confluent.disk.throughput.limit = 10485760000
	confluent.disk.throughput.quota.tier.archive = 1048576000
	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.e2e_checksum.protection.enabled = false
	confluent.e2e_checksum.protection.files = [none]
	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
	confluent.elastic.cku.enabled = false
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.floor.connection.rate.per.ip = -1.0
	confluent.floor.connection.rate.per.tenant = -1.0
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.heap.tenured.notify.bytes = 0
	confluent.heap.tenured.notify.enabled = false
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
	confluent.max.connection.rate.per.ip = -1.0
	confluent.max.connection.rate.per.tenant = -1.0
	confluent.max.connection.throttle.ms = null
	confluent.max.segment.ms = 9223372036854775807
	confluent.metadata.active.encryptor = null
	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.required = false
	confluent.metadata.encryptor.secret.file = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.jvm.warmup.ms = 60000
	confluent.metadata.leader.balance.slice.delay.ms = 100
	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
	confluent.metadata.max.leader.balance.changes.per.slice = 1000
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
	confluent.min.acks = 0
	confluent.min.connection.throttle.ms = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 20000
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.authorizer.enable.acl.state = false
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.lkc.id.enable = false
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.external.listener.name = EXTERNAL
	confluent.network.health.manager.externalconnectivitystartup.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
	confluent.offsets.topic.placement.constraints = 
	confluent.omit.network.processor.metric.tag = false
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.mode.local.default = false
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.replication.mode = PULL
	confluent.replication.push.feature.enable = false
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = true
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.roll.check.interval.ms = 300000
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.config = 
	confluent.security.revoked.certificate.ids = 
	confluent.segment.eager.roll.enable = false
	confluent.segment.speculative.prefetch.enable = false
	confluent.spiffe.id.principal.extraction.rules = 
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.step.connection.rate.per.ip = -1.0
	confluent.step.connection.rate.per.tenant = -1.0
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.subdomain.prefix = null
	confluent.subdomain.separator.map = null
	confluent.subdomain.separator.variable = %sep
	confluent.system.time.roll.enable = false
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.async.enable = false
	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.security.providers = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.provider = null
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.data.loss.validation.fencing.enable = false
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.head.data.loss.validation.enable = true
	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
	confluent.tier.topic.materialization.from.snapshot.enable = false
	confluent.tier.topic.producer.enable.idempotence = true
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.policy.use.computed.assignments = false
	confluent.topic.replica.assignor.builder.class = 
	confluent.track.api.key.per.ip = false
	confluent.track.per.ip.max.size = 100000
	confluent.track.tenant.id.per.ip = false
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	confluent.zookeeper.metadata.migration.controller.check.disable = false
	confluent.zookeeper.metadata.migration.trigger.file.path = null
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [101@controller1:9093, 102@controller2:9093, 103@controller3:9093]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	eligible.leader.replicas.enable = false
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
	group.consumer.heartbeat.interval.ms = 5000
	group.consumer.max.heartbeat.interval.ms = 15000
	group.consumer.max.session.timeout.ms = 60000
	group.consumer.max.size = 2147483647
	group.consumer.min.heartbeat.interval.ms = 5000
	group.consumer.min.session.timeout.ms = 45000
	group.consumer.session.timeout.ms = 45000
	group.coordinator.new.enable = false
	group.coordinator.rebalance.protocols = [classic]
	group.coordinator.threads = 1
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.7-IV4
	k2.topic.creation.enabled = false
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = CONTROLLER://controller1:9093
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /var/lib/kafka/data
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.local.retention.bytes = -2
	log.local.retention.ms = -2
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.after.max.ms = 9223372036854775807
	log.message.timestamp.before.max.ms = 9223372036854775807
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.connections.per.tenant = 0
	max.connections.protected.listeners = []
	max.connections.reap.amount = 0
	max.incremental.fetch.session.cache.slots = 1000
	max.request.partition.size.limit = 2000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.authorizer.support.resource.ids = false
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 101
	nominal.topic.creation.enabled = false
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.custom.metadata.max.bytes = 128
	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = rlmm.config.
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = rsm.config.
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	server.max.startup.time.ms = 9223372036854775807
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.allow.dn.changes = false
	ssl.allow.san.changes = false
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	telemetry.max.bytes = 1048576
	throughput.quota.window.num = 11
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.metadata.load.threads = 32
	transaction.partition.verification.enable = true
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	unstable.api.versions.enable = false
	unstable.metadata.versions.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.metadata.migration.min.batch.size = 200
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2025-07-28 05:43:30,851] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager)
[2025-07-28 05:43:31,025] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager)
[2025-07-28 05:43:31,037] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2025-07-28 05:43:31,050] INFO [ControllerServer id=101] Starting controller (kafka.server.ControllerServer)
[2025-07-28 05:43:31,112] INFO [ControllerServer id=101] FIPS mode enabled: false (kafka.server.ControllerServer)
[2025-07-28 05:43:31,230] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
 (io.confluent.security.audit.AuditLogConfig)
[2025-07-28 05:43:31,231] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
 (io.confluent.security.audit.AuditLogConfig)
[2025-07-28 05:43:31,382] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (io.confluent.crn.CrnAuthorityConfig)
[2025-07-28 05:43:31,386] INFO CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP
 (io.confluent.crn.CrnAuthorityConfig)
[2025-07-28 05:43:31,394] INFO MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false
 (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
[2025-07-28 05:43:31,396] INFO AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.authentication.event.rate.limit = -1
	confluent.security.event.logger.authorization.event.rate.limit = -1
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.logger.kafka.request.rate.limit = -1
	confluent.security.event.logger.physical.cluster.id = 
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 
 (io.confluent.security.audit.AuditLogConfig)
[2025-07-28 05:43:31,397] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
[2025-07-28 05:43:32,853] INFO [SocketServer listenerType=CONTROLLER, nodeId=101] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2025-07-28 05:43:32,964] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
[2025-07-28 05:43:32,970] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
[2025-07-28 05:43:32,972] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
[2025-07-28 05:43:32,978] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2025-07-28 05:43:33,050] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2025-07-28 05:43:33,150] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2025-07-28 05:43:33,162] INFO Config values: 
	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
	confluent.security.event.logger.enable.detailed.audit.logs = false
	confluent.security.event.logger.enable.produce.consume.audit.logs = false
 (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
[2025-07-28 05:43:33,179] INFO [SocketServer listenerType=CONTROLLER, nodeId=101] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2025-07-28 05:43:33,296] INFO [SharedServer id=101] Starting SharedServer (kafka.server.SharedServer)
[2025-07-28 05:43:34,074] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2025-07-28 05:43:34,078] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2025-07-28 05:43:34,078] INFO [MergedLog partition=__cluster_metadata-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2025-07-28 05:43:34,126] INFO Initialized snapshots with IDs SortedSet() from /var/lib/kafka/data/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2025-07-28 05:43:34,174] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2025-07-28 05:43:34,374] INFO [RaftManager id=101] Completed transition to Unattached(epoch=0, voters=[101, 102, 103], electionTimeoutMs=1539) from null (org.apache.kafka.raft.QuorumState)
[2025-07-28 05:43:34,391] INFO [kafka-101-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
[2025-07-28 05:43:34,398] INFO [kafka-101-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2025-07-28 05:43:34,506] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:34,517] INFO [RaftManager id=101] Registered the listener org.apache.kafka.image.loader.MetadataLoader@85878114 (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:34,523] INFO [ControllerServer id=101] Waiting for controller quorum voters future (kafka.server.ControllerServer)
[2025-07-28 05:43:34,523] INFO [ControllerServer id=101] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
[2025-07-28 05:43:34,611] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:34,692] INFO [ControllerServer id=101] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk. (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:34,699] INFO [RaftManager id=101] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1126481887 (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:34,718] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:34,749] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2025-07-28 05:43:34,760] INFO [controller-101-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-07-28 05:43:34,762] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2025-07-28 05:43:34,763] INFO [controller-101-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-07-28 05:43:34,772] INFO [controller-101-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-07-28 05:43:34,774] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
[2025-07-28 05:43:34,806] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2025-07-28 05:43:34,810] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
[2025-07-28 05:43:34,810] INFO [controller-101-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-07-28 05:43:34,821] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:34,823] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2025-07-28 05:43:34,829] INFO [controller-101-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2025-07-28 05:43:34,911] INFO [ExpirationReaper-101-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2025-07-28 05:43:34,922] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:34,927] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,931] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,936] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,937] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,942] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,944] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,948] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:34,949] INFO Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.enable.slowlog = false
	confluent.request.log.samples.per.min = 0
	confluent.request.slowlog.threshold.override = -1.0
	confluent.request.slowlog.threshold.p99.min = -1.0
 (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
[2025-07-28 05:43:35,025] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,056] INFO EventEmitterConfig values: 
 (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
[2025-07-28 05:43:35,125] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,177] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
[2025-07-28 05:43:35,178] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig)
[2025-07-28 05:43:35,226] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,281] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[2025-07-28 05:43:35,329] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,362] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
[2025-07-28 05:43:35,387] INFO ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.cluster.id = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null
 (io.confluent.telemetry.ConfluentTelemetryConfig)
[2025-07-28 05:43:35,389] INFO VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000
 (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[2025-07-28 05:43:35,390] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.contentType = null
	client.metrics.path.override = /v1/metrics
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = null
	proxy.password = null
	proxy.url = null
	proxy.username = null
	remote.configurable = false
	type = http
 (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[2025-07-28 05:43:35,390] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[2025-07-28 05:43:35,390] INFO PollingRemoteConfigurationConfig values: 
	enabled = true
	refresh.interval.ms = 60000
 (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
[2025-07-28 05:43:35,390] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
[2025-07-28 05:43:35,405] INFO EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
 (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
[2025-07-28 05:43:35,429] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,438] INFO HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http
 (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
[2025-07-28 05:43:35,535] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,636] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,736] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,781] INFO [RaftManager id=101] Completed transition to CandidateState(localId=101, epoch=1, retries=1, voteStates={101=GRANTED, 102=UNRECORDED, 103=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1330) from Unattached(epoch=0, voters=[101, 102, 103], electionTimeoutMs=1539) (org.apache.kafka.raft.QuorumState)
[2025-07-28 05:43:35,791] INFO [ControllerServer id=101] In the new epoch 1, the leader is (none). (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:35,843] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:35,943] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,043] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,044] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,053] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,053] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,054] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,111] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,114] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,115] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,115] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,149] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,229] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,230] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,233] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,233] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,255] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,360] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,433] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,441] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,462] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,484] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,484] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,564] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,667] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,768] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,868] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:36,879] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,882] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,884] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,888] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:36,968] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,069] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,107] INFO [RaftManager id=101] Election has timed out, backing off for 0ms before becoming a candidate again (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:37,108] INFO [RaftManager id=101] Re-elect as candidate after election backoff has completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:37,115] INFO [RaftManager id=101] Completed transition to CandidateState(localId=101, epoch=2, retries=2, voteStates={101=GRANTED, 102=UNRECORDED, 103=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1120) from CandidateState(localId=101, epoch=1, retries=1, voteStates={101=GRANTED, 102=UNRECORDED, 103=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1330) (org.apache.kafka.raft.QuorumState)
[2025-07-28 05:43:37,120] INFO [ControllerServer id=101] In the new epoch 2, the leader is (none). (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:37,170] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,274] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,398] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,406] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,408] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,410] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,413] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,498] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,600] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,702] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,802] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,866] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,866] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,904] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:37,932] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:37,936] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,008] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,109] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,216] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,229] INFO [RaftManager id=101] Election has timed out, backing off for 200ms before becoming a candidate again (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:38,317] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,339] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
[2025-07-28 05:43:38,419] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,429] INFO [RaftManager id=101] Re-elect as candidate after election backoff has completed (org.apache.kafka.raft.KafkaRaftClient)
[2025-07-28 05:43:38,441] INFO [RaftManager id=101] Completed transition to CandidateState(localId=101, epoch=3, retries=3, voteStates={101=GRANTED, 102=UNRECORDED, 103=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1719) from CandidateState(localId=101, epoch=2, retries=2, voteStates={101=GRANTED, 102=UNRECORDED, 103=UNRECORDED}, highWatermark=Optional.empty, electionTimeoutMs=1120) (org.apache.kafka.raft.QuorumState)
[2025-07-28 05:43:38,442] INFO [ControllerServer id=101] In the new epoch 3, the leader is (none). (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:38,447] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,448] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,448] INFO [RaftManager id=101] Node 103 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,448] WARN [RaftManager id=101] Connection to node 103 (controller3/172.18.0.3:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,519] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,560] INFO [ControllerServer id=101] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer)
[2025-07-28 05:43:38,567] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,567] INFO [ControllerServer id=101] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,567] INFO [ControllerServer id=101] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,567] INFO [SocketServer listenerType=CONTROLLER, nodeId=101] Enabling request processing. (kafka.network.SocketServer)
[2025-07-28 05:43:38,574] INFO Awaiting socket connections on controller1:9093. (kafka.network.DataPlaneAcceptor)
[2025-07-28 05:43:38,630] INFO [controller-101-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,631] INFO [ControllerServer id=101] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
[2025-07-28 05:43:38,635] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] initialized channel manager. (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:38,636] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] maybeSendControllerRegistration: cannot register yet because the metadata version is still 3.0-IV1, which does not support KIP-919 controller registration. (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:38,664] INFO Kafka version: 7.7.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[2025-07-28 05:43:38,664] INFO Kafka commitId: e42eb4aa8aba3e1cdae0b5edc71b43747ca85a6d (org.apache.kafka.common.utils.AppInfoParser)
[2025-07-28 05:43:38,665] INFO Kafka startTimeMs: 1753681418664 (org.apache.kafka.common.utils.AppInfoParser)
[2025-07-28 05:43:38,666] INFO [KafkaRaftServer nodeId=101] Kafka Server started (kafka.server.KafkaRaftServer)
[2025-07-28 05:43:38,669] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,770] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,871] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,971] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:38,973] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:38,975] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:39,074] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,129] INFO [RaftManager id=101] Completed transition to Leader(localId=101, epoch=3, epochStartOffset=0, highWatermark=Optional.empty, voterStates={101=ReplicaState(nodeId=101, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true), 102=ReplicaState(nodeId=102, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=false), 103=ReplicaState(nodeId=103, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=false)}) from CandidateState(localId=101, epoch=3, retries=3, voteStates={101=GRANTED, 102=UNRECORDED, 103=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1719) (org.apache.kafka.raft.QuorumState)
[2025-07-28 05:43:39,138] INFO [controller-101-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node controller1:9093 (id: 101 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-07-28 05:43:39,179] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,280] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,382] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,446] INFO [RaftManager id=101] Node 102 disconnected. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:39,446] WARN [RaftManager id=101] Connection to node 102 (controller2/172.18.0.4:9093) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
[2025-07-28 05:43:39,483] INFO [MetadataLoader id=101] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,535] INFO [RaftManager id=101] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=106)]) for the first time for epoch 3 based on indexOfHw 1 and voters [ReplicaState(nodeId=101, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=106)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true), ReplicaState(nodeId=103, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=106)])], lastFetchTimestamp=1753681419532, lastCaughtUpTimestamp=1753681419532, hasAcknowledgedLeader=true), ReplicaState(nodeId=102, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=false)] (org.apache.kafka.raft.LeaderState)
[2025-07-28 05:43:39,560] INFO [MetadataLoader id=101] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,567] INFO [ControllerServer id=101] Becoming the active controller at epoch 3, next write offset 1. (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:39,583] INFO [MetadataLoader id=101] initializeNewPublishers: The loader finished catching up to the current high water mark of 1 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,591] WARN [ControllerServer id=101] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 3.7-IV4 from bootstrap source 'the binary bootstrap metadata file: /var/lib/kafka/data/bootstrap.checkpoint'. Setting the ZK migration state to NONE since this is a de-novo KRaft cluster. (org.apache.kafka.controller.QuorumController)
[2025-07-28 05:43:39,602] INFO [ControllerServer id=101] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager)
[2025-07-28 05:43:39,602] INFO [ControllerServer id=101] Replayed a Confluent FeatureLevelRecord setting metadata version to 3.7-IV4 (org.apache.kafka.controller.FeatureControlManager)
[2025-07-28 05:43:39,613] INFO [ControllerServer id=101] Replayed EndTransactionRecord() at offset 4. (org.apache.kafka.controller.OffsetControlManager)
[2025-07-28 05:43:39,633] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,634] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,635] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,637] INFO [ControllerServer id=101] Loaded new metadata Features(version=3.0-IV1, finalizedFeatures={confluent.metadata.version=101}, finalizedFeaturesEpoch=0). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
[2025-07-28 05:43:39,638] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,638] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,638] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=101 with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,639] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=101 with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,639] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] maybeSendControllerRegistration: cannot register yet because the metadata version is still 3.0-IV1, which does not support KIP-919 controller registration. (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:39,647] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing ScramPublisher controller id=101 with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,649] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=101 with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,657] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,658] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing CellMetadataMetricsPublisher with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,658] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,660] INFO [MetadataLoader id=101] InitializeNewPublishers: initializing AclPublisher controller id=101 with a snapshot at offset 0 (org.apache.kafka.image.loader.MetadataLoader)
[2025-07-28 05:43:39,684] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,684] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,685] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,688] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,689] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,689] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,689] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,691] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,692] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,728] INFO [ControllerServer id=101] Loaded new metadata Features(version=3.7-IV4, finalizedFeatures={confluent.metadata.version=119}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
[2025-07-28 05:43:39,729] INFO SBC Event SbcMetadataUpdateEvent-6 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-7]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,730] INFO Handling event SbcConfigUpdateEvent-7 (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,733] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
[2025-07-28 05:43:39,733] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=101, incarnationId=PFtXSekCQrqv8xwa9Ln8_g, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='controller1', port=9093, securityProtocol=0)], features=[Feature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=119), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=19)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:39,804] INFO [ControllerServer id=101] Node 101 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,808] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,811] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,819] INFO [ControllerServer id=101] Replayed RegisterControllerRecord contaning ControllerRegistration(id=101, incarnationId=PFtXSekCQrqv8xwa9Ln8_g, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='controller1', port=9093)], supportedFeatures={confluent.metadata.version: 1-119, metadata.version: 1-19}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,888] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:39,890] INFO [ControllerRegistrationManager id=101 incarnation=PFtXSekCQrqv8xwa9Ln8_g] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
[2025-07-28 05:43:39,919] INFO [ControllerServer id=101] Node 103 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,920] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,920] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:39,920] INFO [ControllerServer id=101] Replayed RegisterControllerRecord contaning ControllerRegistration(id=103, incarnationId=X1meh7RSQTOhtjNvE58H3g, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='controller3', port=9093)], supportedFeatures={confluent.metadata.version: 1-119, metadata.version: 1-19}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:40,449] INFO [ControllerServer id=101] Node 102 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:40,449] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:40,449] INFO [ControllerServer id=101] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
[2025-07-28 05:43:40,450] INFO [ControllerServer id=101] Replayed RegisterControllerRecord contaning ControllerRegistration(id=102, incarnationId=ViftgjkqTciDChaaTzhbuw, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='controller2', port=9093)], supportedFeatures={confluent.metadata.version: 1-119, metadata.version: 1-19}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
